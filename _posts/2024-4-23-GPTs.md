---
title: GPT
date: 2024-04-23 19:00:00 +/-TTTT
categories: [AI,NLP]
tags: [AI,paper-learing,NLP,GPT ]  # TAG names should always be lowercase
---
## Generative Pre-Train 
### Improving Language Understanding by Generative Pre-Training
#### Introduce
* Unlabeled text 
  * difficuty
    * Optimization objective 没有一个目标函数能够适应所以任务，每个目标函数都有对应的任务
    * 如何将学到的表示传递给子任务
* self-supervised
* Transformerr
#### FrameWork
* Unsupervised pre-train
  *  根据之前的k（context）个词 预测下一个词
  *  
  *  相较于BERT，更难的任务，更难训练，但是能力更强
![alt text](https://raw.githubusercontent.com/huazZeng/huazZeng.github.io/main/_posts/img/GPT-1-fineturn.png)
* Supervised fine-turn
  ![alt text](https://raw.githubusercontent.com/huazZeng/huazZeng.github.io/main/_posts/img/GPT-1-objection.png)
* Task-specific input transformations
  * 分类
  * 蕴含
  * 相似
  * 选择题
#### Experience

### GPT-2
**扩大参数量**
#### Abstract
* new dataset : WebText
* Zero-shot 在下游任务 不使用标记过的数据来微调
#### Introduce
* Zero-shot setting
#### Framework
* prompt
* Dataset ： redit

### GPT-3 Few-Shot learner
#### Abstract
* 在下游任务不做微调和更新
* 生成人类难以辨别的文章

#### Introduce
* 在预训练模型应用在下游任务时存在问题
  * 需要大的数据集
  * 微调后的效果好 不代表模型的预训练结果好 可能是过拟合到预训练文本上，预训练文本与下游任务文本重合度大
  * 人类并不需要大量数据集来学习
* Meta-learning
* in-context-learning
* 不同shot-learning 的比较
#### Framework
* Sparse Transformer
* Dataset Commom Crawl
    * 基于GPT-2的内容 对commom crawl 做二分类，正类留下
    * 对文本进行相似度判断，去重
* Beansearch 来得到生成答案

#### Potential Impact
- Potential Misuse
- Fair Bias Reprecentation：gender ，race，religion
- Energy use


### GPT-4 Tech report
* Predicatable scalling
  * OpenAI 可以从在小规模模型的训练上预测大模型的训练结果，减少成本
  * 通过小模型的结果 外推
  * 并不是所有的任务都是如此，大模型的涌现现象仍然存在（Hindsight negiect）
* RLHF
* Visual inputs
* Steerability
  * System message :定义AI的身份

### Append
* zero-shot ： 预训练完模型后 不需要样本直接适应下游任务
* one-shot ：预训练完模型后 使用一个样本直接适应下游任务
* few-shot ：预训练完模型后 采样部分样本直接适应下游任务


### Append.疯狂三月
* Chatgpt plugin ToolFormer
* LLaMA
* Visual Chatgpt
* GigaGAN
* Stanford Alpaca
### 计划
5.5结束
* GPT模型
* 训练方法
* GPT-4多模态


