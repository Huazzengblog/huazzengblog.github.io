---
title: llm_testing
date: 2025-03-01 00:45:00 +/-TTTT
categories: 25spring
tags: [llm,testing]
---
# llm-testing
## paper reading
### LISP（LLM Based Input Space Partitioning Testing for Library APIs）
#### method
* Source code → Input space (Specification).
* Code + Specification → Mappings between parameters and constructor sequences used for instantiation.
* Specifications and selected constructors → Statements.
![alt text](https://raw.githubusercontent.com/huazZengblog/huazZengblog.github.io/main/_posts/img/llm_testing/LISP.png)





#### Motivation:
* LLMs have demonstrated promising capabilities.
* Cons in existing techniques:

  * Search-based testing:
    Generates a large amount of inputs that go through redundant program paths.
  * Symbolic execution:
    Requires heavy computational resources to solve path conditions.

#### Evaluation:

* Code coverage:
    * Average code coverage.
    * Quality.
    * Efficiency.
* Usefulness:Exceptions & Vulnerabilities.
* Cost.
* Ablation Study.


#### Limits and Future:

* Interpretability challenges.
* Complicated API interactions.
* Current reliance on drivers.
* Document-enhanced prompt engineering.

#### Thinking:

* Semantic docs generated by LLM.
* LISP-CG (?).
* Testing oracles.
* "Do not interrupt the thought chain."
* Testing example factory.



### LLM for Test Script Generation and Migration: Challenges, Capabilities, and Opportunities
*  test script
*  focus on mobile apps
#### motivation
  * the compatibility of test scripts with evolving app versions
  * the vast array of mobile devices, operating system versions, and screen sizes adds complexity to test script generation and maintenance
#### RQ
  * generating test scripts based on the natural language descriptions on specific app scenarios
    * **PROBLEM** : 
      * unexpected failure
        * **APPROACH** : The dialogue is partitioned into three phases: initiation, exploration, and summarization.
      * loses track of the prior context --->unstopped
  * migration across different platforms for the same app
  * migrating test scripts among different apps sharing similar functionalities

#### thinking
* multi-modal LLM
* 


### Do LLMs generate test oracles that capture the  actual or the expected program behaviour?
#### conclusion
*   capture the actual program behaviour rather than the expected one. 
*  better at generating test oracles rather than classifying the correct ones
> can generate better test oracles when the code contains meaningful test or variable names. 
*  Finally, LLMgenerated test oracles have higher fault detection potential than the Evosuite ones.
> Mutation testing [24] is a test adequacy criterion

#### RQs
* actual program behaviour or  the expected one. 
  * four type
  * dataset:generated by TOGA  and TOGLL  
  > μBERT [25], to generate invalid code versions, and GPT [28] to generate invalid test oracle assertions 
* What can influence the finding of an expected oracle?
* Can LLMs generate test oracles capturing the expected program behaviour
* How strong are the oracles generated by the LLM?(Mutation)



### Test Oracle Automation in the era of LLMs

* 	潜力
	•	包括超越测试断言的Oracle，如合约或变形关系。
* 挑战
	•	通过提示工程 (Prompt Engineering) 使用LLMs
	•	通过预训练或微调使用LLMs
*	主要威胁
	•	由LLMs生成不同类型的Oracle带来的Oracle缺陷
	•	与数据泄露相关的隐私问题


### Automated Unit Test Improvement using Large Language Models at Meta
> We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.


### Are LLMs Ready for Practical  Adoption for Assertion Generation?
![alt text](https://raw.githubusercontent.com/huazZengblog/huazZengblog.github.io/main/_posts/img/llm_testing/assertion-llm1.png) 
![alt text](https://raw.githubusercontent.com/huazZengblog/huazZengblog.github.io/main/_posts/img/llm_testing/assertion-llm2.png)

### FANDANGO: Evolving Language-Based Testing
* 启发式搜索：遗传算法
* language learner : 
  * 获取grammar 和 constraints 
* 通过grammar 和 constraints  来生产初始的测试用例
* 设置fitness function ：
  $ f(T) = \sum_{c \in constraints} w_c * s_c(T) $
> where:  • wc is the weight assigned to constraint Y, refecting its importance. • 
> Sc  in [0, 1] is the satisfaction score of tree T with respect to constraint c.



## point
* mutation 
* 差分分析
* 3.28
  * think1
    * p -- > 语义理解
    * semetic -- > test set
    * test set -- > p

  * 如何捕获预期的程序行为？
  * 捕获程序在上下文程序中的语义？
    * masked掉部分代码


### dataset

## paper-reading-0606
### paper-related

|  Paper Title                                                                                                    | finished |
| :------------------------------------------------------------------------------------------------------------------------- | :--------- |
| Adversarial Reasoning for Repair Based on Inferred Program Intent                                                          | 1         |
| Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks                      | 1        |
| OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification                            | 1         |
| SpecGen: Automated Generation of Formal Program Specifications via Large Language Models                                   | 1         |
| SpecRover: Code Intent Extraction via LLMs                                                                                 | 1         |
| On Learning Meaningful Assert Statements for Unit Test Cases                                                               | 0         |
| FormalSpecCpp: A Dataset of C++ Formal Specifications created using LLMs                                                   | 0 |
| How Accurately Do Large Language Models Understand Code?                                                                   | 1 |
| How Effective are Large Language Models in Generating Software Specifications?                                             | 1 |
| Intention is All You Need: Refining Your Code from Your Intention                                                          | 1 |
| Seeking Specifications: The Case for Neuro-Symbolic Specification Synthesis                                                | 0 |
| Towards the LLM-Based Generation of Formal Specifications from Natural-Language Contracts: Early Experiments with Symboleo | 0 |
｜Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of LLMs on Formal Specification Inference｜ 1 ｜

### paper-detail
* SpecGen: Automated Generation of Formal Program Specifications via Large Language Models
![alt text](<截屏2025-06-06 09.11.38_副本.png>)
* Repair
  * SpecRover: Code Intent Extraction via LLMs
    * https://arxiv.org/abs/2408.02232
    * ![alt text](<截屏2025-06-06 10.15.48.png>)
  * ADVERINTENT-AGENT
    * https://arxiv.org/pdf/2505.13008
    ![alt text](<截屏2025-06-06 09.11.38_副本-1.png>)
* benchmark
  * Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks
  * Can LLMs Reason About Program Semantics?  A Comprehensive Evaluation of LLMs on Formal Specification Inference
    * FormalBench
  * atlas 
    * https://sites.google.com/view/atlas-nmt/home
    * https://arxiv.org/pdf/2002.05800
  * OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification
    ```json
    [
    {
        "syscall": "syscall name",
        "declaration": "syscall declaration",
        "description": "\n[Functional Description]: ... ",
        "code": "[Code Implementation]: ... ",
        "bug_type": "bug type",
        "bug_num": "bug number"
    },
    ...
    ]
    ```

* code-understanding
  * How Accurately Do Large Language Models Understand Code
    ![alt text](<截屏2025-06-06 10.38.40.png>)
  * How Effective are Large Language Models in Generating Software Specifications?
    > Therefore, we manually inspect the generated completions for Jdoctor-data and report both the raw accuracy of perfect match and final accuracy after manual corrections.


* 通过上下文推断隐含寓意
    * 验证自然语言准确性
    * 验证代码spec的工具