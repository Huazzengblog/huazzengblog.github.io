---
title: llm_testing
date: 2025-03-01 00:45:00 +/-TTTT
categories: 25spring
tags: [llm,testing]
---
# llm-testing
## paper reading
### LISP（LLM Based Input Space Partitioning Testing for Library APIs）
#### method
* Source code → Input space (Specification).
* Code + Specification → Mappings between parameters and constructor sequences used for instantiation.
* Specifications and selected constructors → Statements.
![alt text](https://raw.githubusercontent.com/huazZengblog/huazZengblog.github.io/main/_posts/img/llm_testing/LISP.png)





#### Motivation:
* LLMs have demonstrated promising capabilities.
* Cons in existing techniques:

  * Search-based testing:
    Generates a large amount of inputs that go through redundant program paths.
  * Symbolic execution:
    Requires heavy computational resources to solve path conditions.

#### Evaluation:

* Code coverage:
    * Average code coverage.
    * Quality.
    * Efficiency.
* Usefulness:Exceptions & Vulnerabilities.
* Cost.
* Ablation Study.


#### Limits and Future:

* Interpretability challenges.
* Complicated API interactions.
* Current reliance on drivers.
* Document-enhanced prompt engineering.

#### Thinking:

* Semantic docs generated by LLM.
* LISP-CG (?).
* Testing oracles.
* "Do not interrupt the thought chain."
* Testing example factory.



### LLM for Test Script Generation and Migration: Challenges, Capabilities, and Opportunities
*  test script
*  focus on mobile apps
#### motivation
  * the compatibility of test scripts with evolving app versions
  * the vast array of mobile devices, operating system versions, and screen sizes adds complexity to test script generation and maintenance
#### RQ
  * generating test scripts based on the natural language descriptions on specific app scenarios
    * **PROBLEM** : 
      * unexpected failure
        * **APPROACH** : The dialogue is partitioned into three phases: initiation, exploration, and summarization.
      * loses track of the prior context --->unstopped
  * migration across different platforms for the same app
  * migrating test scripts among different apps sharing similar functionalities

#### thinking
* multi-modal LLM
* 


### Do LLMs generate test oracles that capture the  actual or the expected program behaviour?
#### conclusion
*   capture the actual program behaviour rather than the expected one. 
*  better at generating test oracles rather than classifying the correct ones
> can generate better test oracles when the code contains meaningful test or variable names. 
*  Finally, LLMgenerated test oracles have higher fault detection potential than the Evosuite ones.
> Mutation testing [24] is a test adequacy criterion

#### RQs
* actual program behaviour or  the expected one. 
  * four type
  * dataset:generated by TOGA  and TOGLL  
  > μBERT [25], to generate invalid code versions, and GPT [28] to generate invalid test oracle assertions 
* What can influence the finding of an expected oracle?
* Can LLMs generate test oracles capturing the expected program behaviour
* How strong are the oracles generated by the LLM?(Mutation)



### Test Oracle Automation in the era of LLMs

* 	潜力
	•	包括超越测试断言的Oracle，如合约或变形关系。
* 挑战
	•	通过提示工程 (Prompt Engineering) 使用LLMs
	•	通过预训练或微调使用LLMs
*	主要威胁
	•	由LLMs生成不同类型的Oracle带来的Oracle缺陷
	•	与数据泄露相关的隐私问题


### Automated Unit Test Improvement using Large Language Models at Meta
> We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.


### Are LLMs Ready for Practical  Adoption for Assertion Generation?
![alt text](https://raw.githubusercontent.com/huazZengblog/huazZengblog.github.io/main/_posts/img/llm_testing/assertion-llm1.png) 
![alt text](https://raw.githubusercontent.com/huazZengblog/huazZengblog.github.io/main/_posts/img/llm_testing/assertion-llm2.png)



* mutation 
* 差分分析