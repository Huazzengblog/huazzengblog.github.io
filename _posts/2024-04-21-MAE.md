---
title: MAE
date: 2024-04-21 12:00:00 +/-TTTT
categories: [AI,paper-learing]
tags: [AI,paper-learing,CV,pre-train  ]  # TAG names should always be lowercase
---
## MAE
Masked Autoencoder Are Scalable Vison Learners
**生成模型**
**Auto**: data label 来自同一个东西 
### Abstract
* encoder decoder架构
* Masked 75% 的patch ---使得模型能够去学习特征 而不是学习插值

### Conclusion
* 图片与语言的区别
* 潜在影响 ： 社会影响 和 生成结果可能产生的影响


### Introduction
* CV与NLP的区别：
  * 卷积神经网络难以引入mask
  * 图片与语言的Information density不同，语言token的语义较高级，图像patch的表示较低级
  * NLP的编码器只需要一个全连接层 而像素还原难以用一个全连接层来还原 **the decoder design plays a key role in determining the semantic level of the learned latent representations.**
* mask 多数patch
* decoder：非对称的编码器和解码器
* 编码器只看到没被遮住的块 加速训练


### Model
* MASKING
  * random sampling a few patch

* Encoding
  * Based Vit
  * Only Unmasked patch will be encoding
  * **masked patch is ashared, learned vector that indicates the presence of a missing patch to be predicted. We add positional embeddings to all tokens** 

  
* Decoding
  * 较小的架构 计算开销小
  * 最后一层是线性层 投影到像素的数量的长度 reshape 后得到结果 **MSE作为损失函数**


### 实验
