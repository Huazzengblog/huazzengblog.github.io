---
title: ms-swift
date: 2025-07-21 12:00:00 +/-TTTT
categories: [AI,Training]
tags: [AI,LLM,Training ]
---
# ms-swift
ms-swift是魔搭推出的一个训练框架，在这个框架中，有很多文档中未提及的点和需要注意的点，作为自己实践的笔记，记录下来。

## 环境配置
建议直接使用docker或者apptainer镜像进行环境配置，省去一堆麻烦

## Note
### Resize
在qwen2.5vl中，为了对齐vit的输入格式，会对输入的图片进行resize，resize逻辑可见`qwen_vl_utils.smart_resize`，此处有两个参数`MAX_PIXELS`和`MIN_PIXELS`分别表示resize的最大像素和最小像素,这个参数的选择会影响输入图片的分辨率，越高分辨率则需要更多的视觉token。
如果在显存较小的情况下，可以减少`MAX_PIXELS`和`MIN_PIXELS`，但是需要注意的是，减少了分辨率后，模型的性能会下降，特别是grouding和ocr任务，需要根据任务来选择合适的参数。
使用swift进行微调时，对图片会进行默认的resize预处理，如果在执行grouding任务，需要注意bbox的坐标是否需要调整，以下有两种情况：
* 采用**自定义格式**的数据集，那么就需要自己将bbox进行转换，resize后的宽和高可以由qwen_vl_utils.smart_resize得出，从而可计算缩放比。
* 采用SWIFT官方的数据集，那么不需要进行转换，直接使用即可。详见[Swift官方数据集](https://swift.readthedocs.io/zh-cn/latest/Customization/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86.html)中Grouding部分。

#### Resize可调参数
* swift训练时 通过环境变量传入
* 在推理时需要注意，vllm并不会做图像预处理，需要自己预先处理图像到训练时的尺寸以保证结果的准确；transformers中有预处理，但是参数是内置在processor config中的需要修改或者通过环境变量或参数传入

### Deploy
swift中可以使用vllm作为推理引擎进行部署，但是容易报错，建议直接使用vllm部署就好了




