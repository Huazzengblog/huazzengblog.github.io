---
title: NLP-BASE
date: 2024-09-03 22:00:00 +/-TTTT
categories: [AI,NLP]
tags: [AI，concept ]  # TAG names should always be lowercase
---
# NLP-BASE
## 参考文献
- [word2vec 中的数学原理详解 https://www.cnblogs.com/peghoty/p/3857839.html](https://www.cnblogs.com/peghoty/p/3857839.html**)

## N-gram
* 是一个n-1阶的马尔可夫假设
* 模型参数量级别为 O($N^n$) N为词典数量，n为阶数
  * 因此指数级的上升趋势导致模型很难scaleup，同时提点也不高
* 需要使用平滑化的技术来修正模型的缺陷（**可理解为泛化能力极差**）
  * 在count为0时，该情况出现概率即为0吗
  * count覆盖所有情况时，概率就为1吗
* 使用最大似然估计来使得模型收敛


## 神经网络概率模型
**2003 bengio**
* 词向量的引入
* 相对于传统统计模型的优势
  * 词语的相似性能够体现在向量中
  * 自带平滑功能
* 使用one-hot向量来表示一个词（导致模型容易臃肿，可以修正为）

## word2vec
### CBOW 
**Continuious Bag-of-Words Model**
* Hierarchical Softmax
  * 多个分类器在分支上做判断，每个分类器只负责自己的分支
  * 只体现在输出，输入仍然为onehot向量
* Negative Sampling
  * 在原本基础上，只计算选定的负样本，而不计算全部的，提高运行速度
### Skip-gram
**Continuious Skip-gram model**
* Hierarchical Softmax
* Negative Sampling


## glove